{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code splits data into training, testing and validation. Then it applied augmentation on each crop to get balanced classes in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import random\n",
    "import os\n",
    "from os.path import exists\n",
    "import glob\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the count of objects within each class in the json files and creating stratified split to get approx same distribution of classes in each train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(l):\n",
    "    \"\"\"\n",
    "    Parses a JSON file containing annotations of regions in image tiles and counts the occurrences\n",
    "    of different types of plaques.\n",
    "\n",
    "    Args:\n",
    "        l (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with counts of each plaque type. The keys are:\n",
    "              'Cored', 'Diffuse', 'Coarse-Grained', 'CAA'.\n",
    "    \"\"\"\n",
    "    with open(l) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize the dictionary with known plaque types\n",
    "    plaque_dict = {'Cored': 0, 'Diffuse': 0, 'Coarse-Grained': 0, 'CAA': 0}\n",
    "\n",
    "    # Iterate over each tile and its list of annotated regions\n",
    "    for tileId, ele in data.items():\n",
    "        for region in ele:\n",
    "            # Check if the region has a label and a name for the plaque\n",
    "            if \"label\" in region.keys():\n",
    "                if \"name\" in region['label'].keys():\n",
    "                    plaque_type = region['label'][\"name\"]\n",
    "                    # Increment count if plaque type is one of the known types\n",
    "                    if plaque_type in plaque_dict:\n",
    "                        plaque_dict[plaque_type] += 1\n",
    "\n",
    "    return plaque_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data from two paths\n",
    "path1 = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/amy-def-mfg-jsons\"\n",
    "path2 = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/amy-def-mfg-test-jsons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of json file paths\n",
    "test_df = pd.DataFrame({\"json_path\":glob.glob(os.path.join(path1,\"*.json\"))})\n",
    "train_df = pd.DataFrame({\"json_path\":glob.glob(os.path.join(path2,\"*.json\"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class-wise count of objects in each json file\n",
    "for cls in [\"Cored\",\"Diffuse\",\"Coarse-Grained\",\"CAA\"]:\n",
    "    test_df[cls] = test_df[\"json_path\"].apply(lambda l:get_count(l)[cls] )\n",
    "    \n",
    "    \n",
    "for cls in [\"Cored\",\"Diffuse\",\"Coarse-Grained\",\"CAA\"]:\n",
    "    train_df[cls] = train_df[\"json_path\"].apply(lambda l:get_count(l)[cls] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating file_name column \n",
    "test_df[\"file_name\"] = test_df[\"json_path\"].apply(lambda l: l.split(\"/\")[-1].split(\".\")[0])\n",
    "train_df[\"file_name\"] = train_df[\"json_path\"].apply(lambda l: l.split(\"/\")[-1].split(\".\")[0])\n",
    "# creating flag for train (to combine later)\n",
    "train_df[\"flag\"]=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same test files as was used in first interrater study\n",
    "test_files =  [x.split(\"/\")[-1] for x in glob.glob(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi/test\",\"*\"))]\n",
    "test_df[\"flag\"] = test_df[\"file_name\"].apply(lambda l: \"test\" if l in test_files else \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the train and test data\n",
    "combined_df = pd.concat([train_df,test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json_path</th>\n",
       "      <th>Cored</th>\n",
       "      <th>Diffuse</th>\n",
       "      <th>Coarse-Grained</th>\n",
       "      <th>CAA</th>\n",
       "      <th>file_name</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>XE14-004_1_AmyB_1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>XE17-010_1_AmyB_1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           json_path  Cored  Diffuse  \\\n",
       "0  /gladstone/finkbeiner/steve/work/data/npsad_da...      3        6   \n",
       "1  /gladstone/finkbeiner/steve/work/data/npsad_da...      5       13   \n",
       "\n",
       "   Coarse-Grained  CAA          file_name   flag  \n",
       "0              22    0  XE14-004_1_AmyB_1  train  \n",
       "1               9    0  XE17-010_1_AmyB_1  train  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json_path</th>\n",
       "      <th>Cored</th>\n",
       "      <th>Diffuse</th>\n",
       "      <th>Coarse-Grained</th>\n",
       "      <th>CAA</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>XE18-066_1_AmyB_1XE19-037_1_AmyB_1XE11-039_1_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>95</td>\n",
       "      <td>130</td>\n",
       "      <td>105</td>\n",
       "      <td>34</td>\n",
       "      <td>XE14-004_1_AmyB_1XE17-010_1_AmyB_1XE17-030_1_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>207</td>\n",
       "      <td>339</td>\n",
       "      <td>98</td>\n",
       "      <td>45</td>\n",
       "      <td>XE07-047_1_AmyB_1XE07-064_1_AmyB_1XE17-004_1_A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               json_path  Cored  Diffuse  \\\n",
       "flag                                                                       \n",
       "test   /gladstone/finkbeiner/steve/work/data/npsad_da...     26       85   \n",
       "train  /gladstone/finkbeiner/steve/work/data/npsad_da...     95      130   \n",
       "val    /gladstone/finkbeiner/steve/work/data/npsad_da...    207      339   \n",
       "\n",
       "       Coarse-Grained  CAA                                          file_name  \n",
       "flag                                                                           \n",
       "test               22   15  XE18-066_1_AmyB_1XE19-037_1_AmyB_1XE11-039_1_A...  \n",
       "train             105   34  XE14-004_1_AmyB_1XE17-010_1_AmyB_1XE17-030_1_A...  \n",
       "val                98   45  XE07-047_1_AmyB_1XE07-064_1_AmyB_1XE17-004_1_A...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are lot more objects in validation than train (because no of objects vary in each whole slide image)\n",
    "combined_df.groupby([\"flag\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flagging these slides in train data to get better distribution\n",
    "train_list = [\"XE07-064_1_AmyB_1\",\"XE17-039_1_AmyB_1\",\"XE10-045_1_AmyB_1\",\"XE12-016_1_AmyB_1\",\"XE15-022_1_AmyB_1\",\"XE15-039_1_AmyB_1\"]\n",
    "combined_df[\"flag\"]=np.where(combined_df[\"file_name\"].isin(train_list), \"train\",combined_df[\"flag\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json_path</th>\n",
       "      <th>Cored</th>\n",
       "      <th>Diffuse</th>\n",
       "      <th>Coarse-Grained</th>\n",
       "      <th>CAA</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>XE18-066_1_AmyB_1XE19-037_1_AmyB_1XE11-039_1_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>183</td>\n",
       "      <td>258</td>\n",
       "      <td>153</td>\n",
       "      <td>64</td>\n",
       "      <td>XE14-004_1_AmyB_1XE17-010_1_AmyB_1XE17-030_1_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>/gladstone/finkbeiner/steve/work/data/npsad_da...</td>\n",
       "      <td>119</td>\n",
       "      <td>211</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>XE07-047_1_AmyB_1XE17-004_1_AmyB_1XE07-048_1_A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               json_path  Cored  Diffuse  \\\n",
       "flag                                                                       \n",
       "test   /gladstone/finkbeiner/steve/work/data/npsad_da...     26       85   \n",
       "train  /gladstone/finkbeiner/steve/work/data/npsad_da...    183      258   \n",
       "val    /gladstone/finkbeiner/steve/work/data/npsad_da...    119      211   \n",
       "\n",
       "       Coarse-Grained  CAA                                          file_name  \n",
       "flag                                                                           \n",
       "test               22   15  XE18-066_1_AmyB_1XE19-037_1_AmyB_1XE11-039_1_A...  \n",
       "train             153   64  XE14-004_1_AmyB_1XE17-010_1_AmyB_1XE17-030_1_A...  \n",
       "val                50   15  XE07-047_1_AmyB_1XE17-004_1_AmyB_1XE07-048_1_A...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.groupby([\"flag\"]).sum() # The distribution looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folders Train/Val/Test, and copy files based on flag to respective folder (train, test and val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/amyb_wsi_v2\"\n",
    "TARGET_PATH = '/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/amyb_wsi_v2/train' \n",
    "target_folders = [\"images\", \"labels\"]\n",
    "origin_folders = [\"image\",\"mask\"]\n",
    "# keeping patient folder with train flag\n",
    "imgs = combined_df[combined_df[\"flag\"]==\"train\"][\"file_name\"].values\n",
    "for i in imgs:\n",
    "   for target_folder,origin_folder in zip(target_folders,origin_folders):\n",
    "      if i==\".DS_Store\" :\n",
    "         continue\n",
    "      origin = os.path.join(DATASET_PATH,i,origin_folder)\n",
    "      target = os.path.join(TARGET_PATH,target_folder)\n",
    "      if not os.path.exists(target):\n",
    "         os.makedirs(target)\n",
    "      # Fetching the list of all the files\n",
    "      files = os.listdir(origin)\n",
    "      # Fetching all the files to directory\n",
    "      for file_name in files:\n",
    "         shutil.copyfile(os.path.join(origin,file_name), os.path.join(target,file_name))\n",
    "      print(\"Files are copied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n",
      "Files are copied successfully\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/amyb_wsi_v2\"\n",
    "TARGET_PATH = '/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/amyb_wsi_v2/val' \n",
    "target_folders = [\"images\", \"labels\"]\n",
    "origin_folders = [\"image\",\"mask\"]\n",
    "# keeping patient folder with val flag\n",
    "imgs = combined_df[combined_df[\"flag\"]==\"val\"][\"file_name\"].values\n",
    "for i in imgs:\n",
    "   for target_folder,origin_folder in zip(target_folders,origin_folders):\n",
    "      if i==\".DS_Store\" :\n",
    "         continue\n",
    "      origin = os.path.join(DATASET_PATH,i,origin_folder)\n",
    "      target = os.path.join(TARGET_PATH,target_folder)\n",
    "      if not os.path.exists(target):\n",
    "         os.makedirs(target)\n",
    "      # Fetching the list of all the files\n",
    "      files = os.listdir(origin)\n",
    "      # Fetching all the files to directory\n",
    "      for file_name in files:\n",
    "         shutil.copyfile(os.path.join(origin,file_name), os.path.join(target,file_name))\n",
    "      print(\"Files are copied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Augment each crop using a multiplier, which is determined based on the underrepresentation of class objects in the training/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep crop paths of cored, diffuse, coarse-grained and caa classes in a list. This is bit approximated as cored crop list may contain diffuse plaque objects and so on\n",
    "\n",
    "all_images = glob.glob(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/train/images/\",'*.png')) #/amyb_wsi_v2/val/images/ for validation\n",
    "all_masks = glob.glob(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/train/labels/\",'*.png'))\n",
    "\n",
    "all_images.sort()\n",
    "all_masks.sort()\n",
    "\n",
    "cored_images, cored_masks, diffuse_images, diffuse_masks, cg_images,cg_masks, caa_images, caa_masks  = [],[],[],[],[],[],[],[]\n",
    "for image, mask in zip(all_images,all_masks):\n",
    "    found = np.unique(np.array(Image.open(mask)))\n",
    "    # color code for cored: 50\n",
    "    if 50 in found:\n",
    "        cored_images.append(image)\n",
    "        cored_masks.append(mask)\n",
    "        \n",
    "    # color code for diffuse: 100\n",
    "    if 100 in found:\n",
    "        diffuse_images.append(image)\n",
    "        diffuse_masks.append(mask)\n",
    "        \n",
    "    # color code for coarse-grained: 150\n",
    "    if 150 in found:\n",
    "        cg_images.append(image)\n",
    "        cg_masks.append(mask)\n",
    "        \n",
    "    # color code for caa: 200\n",
    "    if 200 in found:\n",
    "        caa_images.append(image)\n",
    "        caa_masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 202, 178, 72)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmentation multiplier is decided based this count. For example this case will use ratio of 1.5:1:1.2:3  for augmentation\n",
    "len(cored_images),  len(diffuse_images), len(cg_images), len(caa_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selected transforms are designed to make minimal changes to the crops\n",
    "transforms  = A.Compose([A.VerticalFlip(p=0.5),\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.Blur(blur_limit=1,p=0.2),\n",
    "                            A.OpticalDistortion(p=0.25),\n",
    "                            A.HueSaturationValue(hue_shift_limit=2, sat_shift_limit=3, val_shift_limit=2, p=0.25),\n",
    "                            A.RandomRotate90(p=0.5),\n",
    "                            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25)\n",
    "                        ])\n",
    "\n",
    "\n",
    "def get_randimages_dataug(total_imgs, image_filenames, label_filenames):\n",
    "    \"\"\"\n",
    "    Randomly selects pairs of image and label filenames for data augmentation.\n",
    "\n",
    "    This function takes a total number of images to select and two lists of image and label filenames.\n",
    "    It then randomly selects pairs of images and their corresponding labels, ensuring that each image \n",
    "    and label is selected based on the same random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        total_imgs (int): The total number of image-label pairs to return.\n",
    "        image_filenames (list of str): A list of file paths for the image files.\n",
    "        label_filenames (list of str): A list of file paths for the label files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing two lists:\n",
    "            - A list of randomly selected image filenames.\n",
    "            - A list of randomly selected label filenames, corresponding to the image filenames.\n",
    "    \"\"\"\n",
    "    # Sort image and label filenames to maintain a consistent order\n",
    "    image_filenames.sort()\n",
    "    label_filenames.sort()\n",
    "    \n",
    "    # Initialize lists to store random image and label filenames\n",
    "    random_image_file = []\n",
    "    random_label_file = []\n",
    "    \n",
    "    # Select random pairs of image and label filenames\n",
    "    for i in range(total_imgs):\n",
    "        random.seed(i)\n",
    "        random_image_file.append(random.choice(image_filenames))\n",
    "        random.seed(i)\n",
    "        random_label_file.append(random.choice(label_filenames))\n",
    "    return [random_image_file, random_label_file]\n",
    "\n",
    "def upsample_dataset(dataset_base_dir, random_img_filenames, rand_label_filenames, variations, transforms, dest_img_folder_name, dest_label_folder_name):\n",
    "    \"\"\"\n",
    "    Augments a dataset by applying random transformations to images and their corresponding masks.\n",
    "    \n",
    "    This function loads images and their corresponding labels from the specified filenames, \n",
    "    applies a series of transformations to each image-mask pair, and saves the augmented \n",
    "    images and labels to new directories.\n",
    "\n",
    "    Args:\n",
    "        dataset_base_dir (str): The base directory of the dataset where images and labels are stored.\n",
    "        random_img_filenames (list of str): A list of file paths for the image files.\n",
    "        rand_label_filenames (list of str): A list of file paths for the label files.\n",
    "        variations (int): The number of augmented variations to generate per image-label pair. This represent augmentation multiplier\n",
    "        transforms (callable): A function or pipeline of transformations to apply to the images and masks.\n",
    "        dest_img_folder_name (str): The name of the folder to save the augmented images.\n",
    "        dest_label_folder_name (str): The name of the folder to save the augmented labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - A list of file paths for the augmented images.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    aug_img_files = []\n",
    "    aug_mask_files = []\n",
    "    random.seed(500)\n",
    "\n",
    "    # Create directories for augmented images and labels if they don't exist\n",
    "    aug_img_dir = os.path.join(dataset_base_dir, dest_img_folder_name)\n",
    "    if not os.path.exists(aug_img_dir):\n",
    "            os.makedirs(aug_img_dir)\n",
    "            print(\"Augmented Directory '%s' created\" %aug_img_dir)\n",
    "            \n",
    "    aug_mask_dir = os.path.join(dataset_base_dir, dest_label_folder_name)\n",
    "    if not os.path.exists(aug_mask_dir):\n",
    "            os.makedirs(aug_mask_dir)\n",
    "            print(\"Augmented Directory '%s' created\" %aug_mask_dir)\n",
    "\n",
    "    print(os.listdir(dataset_base_dir))\n",
    "    print(\"\\nData Augmentation in Progress ...\")\n",
    "    total_imgs = len(random_img_filenames)\n",
    "    \n",
    "    for i in range(total_imgs):\n",
    "        # Load the image and mask\n",
    "        img = Image.open(random_img_filenames[i]).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        mask = Image.open(rand_label_filenames[i]).convert('P')\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        for j in range(variations):\n",
    "            # Apply transformations to the image and mask\n",
    "            transformed = transforms(image=img, mask=mask)\n",
    "            transformed_img = transformed[\"image\"]\n",
    "            transformed_img = Image.fromarray(transformed_img)\n",
    "\n",
    "            #To rename the file with prefix A_\n",
    "            filename = os.path.basename(random_img_filenames[i])\n",
    "            filepath = os.path.dirname(random_img_filenames[i])\n",
    "\n",
    "            aug_file_name = \"A_\" + str(i) + \"_\" + str(j) + \"_\" + filename\n",
    "            new_file = os.path.join(dataset_base_dir,dest_img_folder_name,\n",
    "                                    aug_file_name)\n",
    "            \n",
    "            \n",
    "            transformed_img.save(new_file)\n",
    "            aug_img_files.append(new_file)\n",
    "\n",
    "            # Apply transformation to the mask and save it\n",
    "            transformed_mask = transformed[\"mask\"]\n",
    "            transformed_mask = Image.fromarray(transformed_mask)\n",
    "            \n",
    "            #To rename the file with prefix A_\n",
    "            filename = os.path.basename(rand_label_filenames[i])\n",
    "            filepath = os.path.dirname(rand_label_filenames[i])\n",
    "            aug_file_name = \"A_\" + str(i) + \"_\" + str(j) + \"_\" + filename\n",
    "            new_file = os.path.join(dataset_base_dir, dest_label_folder_name,\n",
    "                                    aug_file_name)\n",
    "            transformed_mask.save(new_file)\n",
    "            aug_mask_files.append(new_file)\n",
    "    return aug_img_files, aug_mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', '.DS_Store', 'images']\n",
      "\n",
      "Data Augmentation in Progress ...\n"
     ]
    }
   ],
   "source": [
    "aug_value = 50\n",
    "rand_image_filenames, rand_label_filenames = get_randimages_dataug(aug_value, cored_images, cored_masks)  # change variables such as for diffuse use - diffuse_images, diffuse_masks and decide aug value based on count \n",
    "dataset_base_dir = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/test_augmentations\"\n",
    "aug_img_files, aug_mask_files = upsample_dataset(dataset_base_dir, rand_image_filenames, rand_label_filenames, 5, transforms, \"images\", \"labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfold_amy_plaque1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
