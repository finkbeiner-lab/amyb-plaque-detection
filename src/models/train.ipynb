{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), *tuple(['..'])))\n",
    "import argparse\n",
    "\n",
    "from typing import Callable, Dict, List, Optional, Set\n",
    "from collections import OrderedDict\n",
    "import pdb\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim\n",
    "import wandb\n",
    "from model_mrcnn import _default_mrcnn_config, build_default\n",
    "from features import build_features\n",
    "from features import transforms as T\n",
    "from utils.engine import evaluate\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from visualization.explain import ExplainPredictions\n",
    "\n",
    "torch.__future__.set_overwrite_module_params_on_conversion(True)\n",
    "\n",
    "def visualize_augmentations(images, targets):\n",
    "\n",
    "    plt.figure(figsize=(10,10)) # specifying the overall grid size\n",
    "    plt.suptitle('Data Augmentations')\n",
    "    plt.subplot(1,2, 1)\n",
    "    \n",
    "\n",
    "    for i in range(len(images)):\n",
    "        display_list = []\n",
    "        img = images[i].detach().cpu().numpy()\n",
    "        display_list.append(img)\n",
    "        mask = targets[i]['masks'].detach().cpu().numpy()\n",
    "        mask = mask.transpose(1, 2, 0)\n",
    "        display_list.append(mask)\n",
    "\n",
    "        for j in range(2):\n",
    "            plt.subplot(1,2,j+1)\n",
    "            plt.imshow(display_list[j])\n",
    "        \n",
    "        save_name = \"../../../reports/figures/augmentation_{img_no}.png\"\n",
    "\n",
    "        plt.savefig(save_name.format(img_no=i))\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: Callable[[Dict[str, Tensor]], Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    epoch: int = 1,\n",
    "    log_freq: int = 10,) -> None:\n",
    "\n",
    "    assert model.training\n",
    "    model_params = set(model.parameters())\n",
    "    model_devices = set([p.device for p in model_params])\n",
    "    assert model_devices == set([device]) # validate model params device\n",
    "    for g in optimizer.param_groups: # validate optimizer params\n",
    "        assert set(g['params']).issubset(model_params)\n",
    "\n",
    "    log_metrics = list()\n",
    "\n",
    "    for i, (images, targets) in enumerate(train_data_loader):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [dict([(k, v.to(device)) for k, v in target.items()]) for target in targets]\n",
    "        # visualize_augmentations(images , targets)\n",
    "        # pdb.set_trace()\n",
    "        optimizer.zero_grad()\n",
    "        loss, metrics = loss_fn(model.forward(images, targets))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log_metrics.append(dict(epoch=epoch, loss=loss.item(), metrics=metrics))\n",
    "        # print(dict(epoch=epoch, loss=loss.item(), metrics=metrics))\n",
    "        print_logs = \"epoch no : {epoch}, batch no : {batch_no}, total loss : {loss},  classifier :{classifier}, mask: {mask} ===================\"\n",
    "        print(print_logs.format(epoch=epoch, batch_no=i, loss=loss.item(),  classifier=metrics['loss_classifier'], mask=metrics['loss_mask']))\n",
    "        if (i % log_freq) == 0:\n",
    "            yield log_metrics\n",
    "            log_metrics = list()\n",
    "\n",
    "    yield log_metrics\n",
    "\n",
    "\n",
    "def get_loss_fn(weights, default=0.):\n",
    "    \n",
    "    def compute_loss_fn(losses):\n",
    "        item = lambda k: (k, losses[k].item())\n",
    "        metrics = OrderedDict(list(map(item, [k for k in weights.keys() if k in losses.keys()] + [k for k in losses.keys() if k not in weights.keys()])))\n",
    "\n",
    "        loss = sum(map(lambda k: losses[k] * (weights[k] if weights is not None and k in weights.keys() else default), losses.keys()))\n",
    "        return loss, metrics\n",
    "    return compute_loss_fn\n",
    "\n",
    "\n",
    "def get_resp(prompt, prompt_fn=None, resps='n y'.split()):\n",
    "    resp = input(prompt)\n",
    "    while resp not in resps:\n",
    "        resp = input(prompt if prompt_fn is None else propt_fn(resp))\n",
    "    return resps.index(resp)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lambda _: tuple(zip(*_)) # one-liner, no need to import\n",
    "dataset_base_dir = '/mnt/new-nas/work/data/npsad_data/vivek/Datasets/amyb_wsi/'\n",
    "dataset_train_location = '/mnt/new-nas/work/data/npsad_data/vivek/Datasets/amyb_wsi/train-short'\n",
    "dataset_test_location = '/mnt/new-nas/work/data/npsad_data/vivek/Datasets/amyb_wsi/train-short'\n",
    "\n",
    "train_config = dict(\n",
    "    epochs = 1,\n",
    "    batch_size = 6,\n",
    "    num_classes = 4,\n",
    "    device_id = 0,\n",
    "    ckpt_freq =500,\n",
    "    eval_freq = 1,\n",
    ")\n",
    "\n",
    "test_config = dict(\n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "model_config = _default_mrcnn_config(num_classes=1 + train_config['num_classes']).config\n",
    "optim_config = dict(\n",
    "    # cls=grad_optim.GradSGD,\n",
    "    cls=torch.optim.SGD,\n",
    "    defaults=dict(lr=1. * (10. ** (-2)))  #-4 is too slow \n",
    ")\n",
    "wandb_config = dict(\n",
    "        project='nps-ad-vivek',\n",
    "        entity='hellovivek',\n",
    "        # mode = 'offline',\n",
    "        config=dict(\n",
    "            train_config=train_config,\n",
    "            model_config=model_config,\n",
    "            optim_config=optim_config,\n",
    "        ),\n",
    "        save_code=False,\n",
    "        group='runs',\n",
    "        job_type='train',\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Dataset loading\n",
    "train_dataset = build_features.AmyBDataset(dataset_train_location, T.Compose([T.ToTensor()]))\n",
    "test_dataset = build_features.AmyBDataset(dataset_test_location, T.Compose([T.ToTensor()]))\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=train_config['batch_size'], shuffle=True, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=test_config['batch_size'], shuffle=False, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Model Building\n",
    "model = build_default(model_config, im_size=1024)\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    assert train_config['device_id'] >= 0 and train_config['device_id'] < torch.cuda.device_count()\n",
    "    device = torch.device('cuda', train_config['device_id'])\n",
    "model = model.to(device)\n",
    "model.train(True)\n",
    "\n",
    "loss_names = 'objectness rpn_box_reg classifier box_reg mask'.split()\n",
    "loss_weights = [1., 4., 1., 4., 1.,]\n",
    "loss_weights = OrderedDict([(f'loss_{name}', weight) for name, weight in zip(loss_names, loss_weights)])\n",
    "\n",
    "loss_fn = get_loss_fn(loss_weights)\n",
    "\n",
    "optimizer = optim_config['cls']([dict(params=list(model.parameters()))], **optim_config['defaults'])\n",
    "\n",
    "run = wandb.init(**wandb_config)\n",
    "assert run is wandb.run # run was successfully initialized, is not None\n",
    "run_id, run_dir = run.id, run.dir\n",
    "exp_name = run.name\n",
    "\n",
    "artifact_name = f'{run_id}-logs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "for epoch in range(train_config['epochs']):\n",
    "    print(f'Epoch {epoch}=======================================>.')\n",
    "\n",
    "    for logs in train_one_epoch(model, loss_fn, optimizer, train_data_loader, device, epoch=epoch, log_freq=1):\n",
    "        print(logs)\n",
    "        #for log in logs:/mnt/new-nas/work/data/npsad_data/vivek/\n",
    "        #    run.log(log)\n",
    "\n",
    "    # if epoch + 1 == train_config['epochs'] or epoch % train_config['ckpt_freq'] == 0:\n",
    "    #     artifact = wandb.Artifact(artifact_name, type='files')\n",
    "    #     with artifact.new_file(f'ckpt/{epoch}.pt', 'wb') as f:\n",
    "    #         torch.save(model.state_dict(), f)\n",
    "    #     run.log_artifact(artifact)\n",
    "\n",
    "    if epoch % train_config['eval_freq'] == 0:\n",
    "        eval1, eval2, eval_res = evaluate(run, model, test_data_loader,False, device=device)\n",
    "        \n",
    "    \n",
    "    model.train(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_save_name = dataset_base_dir + \"/models{name}_mrcnn_model_{epoch}.pth\"\n",
    "torch.save(model.state_dict(), model_save_name.format(name=exp_name, epoch=train_config['epochs']))\n",
    "\n",
    "\n",
    "# print(\"\\n =================The Model is Trained!====================\")\n",
    "# print(\"-----------------Visualizing Model predictions----------------\")\n",
    "\n",
    "# # TODO Testing is done on Individual WSI Folders\n",
    "# input_path = '/mnt/new-nas/work/data/npsad_data/vivek/Datasets/amyb_wsi/test'\n",
    "\n",
    "# model = build_default(model_config, im_size=1024)\n",
    "\n",
    "# explain = ExplainPredictions(model, model_input_path = model_save_name.format(name=exp_name, epoch=train_config['epochs']), test_input_path=input_path, \n",
    "#                             detection_threshold=0.75, wandb=run, save_result=True, ablation_cam=True, save_thresholds=False)\n",
    "# explain.generate_results()\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iouThrs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection wise plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"class\",\"tpr\",\"fpr\",\"recall\",\"precision\"])\n",
    "len_classes = 3 ## Assuming 3 classes\n",
    "for c in range(len_classes): # running for all 3 classes\n",
    "    ## parameters\n",
    "    area_index = 0 # area - all (areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]  -> areaRngLbl = ['all', 'small', 'medium', 'large'])\n",
    "    threshold_index= 0 # threshold 0.5  (iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)) -- can vary threshold from here \n",
    "    iou_type=\"bbox\"\n",
    "    maxDet = 100 \n",
    "    # Selecting from table\n",
    "    eval_table = eval2[iou_type][c][area_index]\n",
    "\n",
    "    dt_score_list = np.concatenate([eval_table[i]['dtScores'][0:maxDet] for i in range(len(eval_table)) if eval_table[i]!=None])\n",
    "    inds = np.argsort(-dt_score_list, kind='mergesort') \n",
    "    dtScoresSorted = dt_score_list[inds]\n",
    "    dtm  = np.concatenate([eval_table[i]['dtMatches'][threshold_index][0:maxDet]  for i in range(len(eval_table)) if eval_table[i]!=None]) [inds]\n",
    "    dtIg  = np.concatenate([eval_table[i]['dtIgnore'][threshold_index][0:maxDet]  for i in range(len(eval_table)) if eval_table[i]!=None]) [inds]\n",
    "    gtIg = np.concatenate([eval_table[i]['gtIgnore'] for i in range(len(eval_table)) if eval_table[i]!=None])\n",
    "    npig = np.count_nonzero(gtIg==0)\n",
    "    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n",
    "    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n",
    "    tp_sum = np.cumsum(tps, axis=0).astype(dtype=np.float)\n",
    "    tp_sum=tp_sum/tp_sum[-1]\n",
    "    fp_sum = np.cumsum(fps, axis=0).astype(dtype=np.float)\n",
    "    fp_sum=fp_sum/fp_sum[-1]\n",
    "    rc_list =[]\n",
    "    pr_list =[]\n",
    "    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n",
    "        rc = tp / npig\n",
    "        pr = tp / (fp+tp+np.spacing(1))\n",
    "        rc_list.append(rc)\n",
    "        pr_list.append(pr)\n",
    "    tmp = pd.DataFrame({\"tpr\":tp_sum,\"fpr\":fp_sum,\"recall\":rc_list,\"precision\":pr_list})\n",
    "    tmp[\"class\"] = c\n",
    "    df = pd.concat([df, tmp])\n",
    "\n",
    "    #plt.plot(rc_list,pr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "for i in range(3):\n",
    "    #name = f\"{i} (AUC={auc_score:.2f})\"\n",
    "    fig.add_trace(go.Scatter(x=df[df[\"class\"]==i][\"fpr\"], y=df[df[\"class\"]==i][\"tpr\"], name=i, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.write_html(\"ROC_Curve.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "#fig.add_shape(\n",
    "#    type='line', line=dict(dash='dash'),\n",
    "#    x0=0, x1=1, y0=1, y1=0\n",
    "#)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    #name = f\"{i} (AUC={auc_score:.2f})\"\n",
    "    fig.add_trace(go.Scatter(x=df[df[\"class\"]==i][\"recall\"], y=df[df[\"class\"]==i][\"precision\"], name=i, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    #yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    #xaxis=dict(constrain='domain'),\n",
    "    width=1000, height=500\n",
    ")\n",
    "fig.write_html(\"PR_Curve.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amy_plague",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
