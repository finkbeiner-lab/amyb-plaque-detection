{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code runs predictions for external dataset 1 and match the output with ground truth of external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "#from models.model_mrcnn import _default_mrcnn_config, build_default\n",
    "import torchvision\n",
    "import torch\n",
    "from models.generalized_mask_rcnn_pl import LitMaskRCNN\n",
    "import torchvision.ops.boxes as bops\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyvips as Vips\n",
    "from data.inference_data.Reinhard import Reinhard\n",
    "from openslide import OpenSlide\n",
    "from PIL import Image, ImageDraw\n",
    "from glob import glob\n",
    "import random\n",
    "from utils.infer_utils import get_outputs_nms, prepare_input\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset =  \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/data/CERAD/1536_tiles\"\n",
    "image_folders =  os.listdir(image_dataset)\n",
    "image_details_csv =  \"/gladstone/finkbeiner/steve//work/data/npsad_data/vivek/Datasets/External_Dataset-Others/gladstone_results/image_details.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_data = pd.read_csv(image_details_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/UCDavis-Dataset/classification_dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"imagename\"] =train_df[\"imagename\"].apply(lambda l: l.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(coords_data,train_df,  on=\"imagename\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data = pd.read_csv(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/data/CERAD/predictions_ucdavis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/runpod_mrcnn_models/yp2mf3i8_epoch=108-step=872.ckpt\"\n",
    "model = LitMaskRCNN.load_from_checkpoint(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess external data input to make it as same format as trained model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_and_crop(image_path, crop_coords, zoom_factor, blob_coords):\n",
    "    \"\"\"\n",
    "    Zoom into an image and crop a specific region.\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - crop_coordinates (tuple): Coordinates for the crop (x, y, width, height).\n",
    "    - zoom_factor (float): Factor by which to zoom into the image.\n",
    "    - output_path (str): Path to save the cropped and zoomed image.\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    # Calculate the new dimensions for the zoomed image\n",
    "    zoomed_width = int(img.width * zoom_factor)\n",
    "    zoomed_height = int(img.height * zoom_factor)\n",
    "    # Resize the image (zoom in or out)\n",
    "    img = img.resize((zoomed_width, zoomed_height), Image.LANCZOS)\n",
    "    # Crop the zoomed image\n",
    "    # Adjust crop coordinates to match the zoomed image\n",
    "    x, y, width, height = crop_coords\n",
    "    zoomed_x = int(x * zoom_factor)\n",
    "    zoomed_y = int(y * zoom_factor)\n",
    "    zoomed_width = int(width * zoom_factor)\n",
    "    zoomed_height = int(height * zoom_factor)\n",
    "    \n",
    "    # crop image\n",
    "    cropped_img = img.crop((zoomed_x, zoomed_y, zoomed_x + zoomed_width, zoomed_y + zoomed_height))\n",
    "    \n",
    "    blob_x, blob_y, blob_w, blob_h = blob_coords\n",
    "\n",
    "    b_zoomed_x = int( (blob_x * zoom_factor)-zoomed_x)\n",
    "    b_zoomed_y = int( (blob_y * zoom_factor)-zoomed_y)\n",
    "    b_zoomed_width = int(blob_w * zoom_factor)\n",
    "    b_zoomed_height = int(blob_h * zoom_factor)\n",
    "    #zoomed_blob_coords = [b_zoomed_x,b_zoomed_y,b_zoomed_width,b_zoomed_height]\n",
    "    scale_x = 1024 / 512\n",
    "    scale_y = 1024 / 512\n",
    "    zoomed_blob_coords = [b_zoomed_x*scale_x,   b_zoomed_y*scale_y,  b_zoomed_x*scale_x + b_zoomed_width*scale_x,  b_zoomed_y*scale_y + b_zoomed_height*scale_y]\n",
    "    cropped_img = cropped_img.resize((1024, 1024), Image.LANCZOS)\n",
    "    return cropped_img, zoomed_blob_coords\n",
    "\n",
    "\n",
    "def change_to_numpy_array(input_tensor):\n",
    "    input_tensor_cpu = input_tensor.squeeze(0).cpu()  # Remove batch dimension and move to CPU if necessary\n",
    "    # Convert to NumPy array\n",
    "    numpy_array = input_tensor_cpu.permute(1, 2, 0).numpy()  # Convert to HWC format for visualization\n",
    "    # Scale back to 0-255 range if needed\n",
    "    numpy_array = (numpy_array * 255).astype(np.uint8)\n",
    "    return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize normalization based on the reference image\n",
    "def initialize_normalizer(ref_slide_path):\n",
    "    \"\"\"\n",
    "    Use Reinhard to normalize\n",
    "    \"\"\"\n",
    "    print(\"Initializing normalization with reference slide:\", ref_slide_path)\n",
    "    ref_image = Vips.Image.new_from_file(ref_slide_path)\n",
    "    normalizer = Reinhard()\n",
    "    normalizer.fit(ref_image)\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "# Function to normalize a single slide\n",
    "def normalize_slide(target_image, normalizer):\n",
    "    \"\"\"\n",
    "    Normalize target image with the normalizer\n",
    "    \"\"\"\n",
    "    #print(\"Normalizing slide:\", target_slide_path)\n",
    "    # Load the target slide\n",
    "    #target_image = Vips.Image.new_from_file(target_slide_path)\n",
    "    target_image = Vips.Image.new_from_memory(target_image.data, target_image.shape[1], target_image.shape[0], 3, \"uchar\")\n",
    "    # Perform normalization\n",
    "    normalized_image = normalizer.transform(target_image)\n",
    "    # Generate the output file path\n",
    "    #file_name = os.path.basename(target_slide_path).replace(\".mrxs\", \".tif\")\n",
    "    #output_file_path = os.path.join(output_dir, idx+\"_\" +filepath.split(\"/\")[-1])\n",
    "    # Save the normalized image\n",
    "    #normalized_image.write_to_file(output_file_path)\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing normalization with reference slide: /gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/XE07-047_1_AmyB_1/image/XE07-047_1_AmyB_1_8381x_120830y_image.png\n"
     ]
    }
   ],
   "source": [
    "# Path to the reference slide\n",
    "ref_slide_path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/XE07-047_1_AmyB_1/image/XE07-047_1_AmyB_1_8381x_120830y_image.png\"\n",
    "# Initialize the normalizer\n",
    "normalizer = initialize_normalizer(ref_slide_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[\"id1\"] = merged_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61424"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_factor = 2\n",
    "def run_prediction(row):\n",
    "    \"\"\"\n",
    "    This functions runs on a each dataframe row \n",
    "    loads image, preprocesses it and then generate predictions \n",
    "    which are returned as a dict labels, matched_labels and scores\n",
    "    \"\"\"\n",
    "    #print(row[\"id1\"], \"... of 61424\")\n",
    "    # get image path\n",
    "    image_path = os.path.join(image_dataset, row[\"source\"], \"0\",str(row[\"tile_row\"]),str(row[\"tile_column\"])+\".jpg\")\n",
    "    \n",
    "    # crop coords\n",
    "    crop_coords = row[\"image coordinates (xywh)\"].replace(\"[\",\"\").replace(\"]\",\"\").split()\n",
    "    crop_coords = [int(x) for x in crop_coords]\n",
    "    \n",
    "    #blob coords\n",
    "    blob_coords = row[\"blob coordinates (xywh)\"].replace(\"[\",\"\").replace(\"]\",\"\").split()\n",
    "    blob_coords = [int(x) for x in blob_coords]\n",
    "\n",
    "    # zoom and crop\n",
    "    cropped_img, zoomed_blob_coords = zoom_and_crop(image_path, crop_coords, zoom_factor,blob_coords)\n",
    "    \n",
    "    # normalize image\n",
    "    norm_img = normalize_slide(np.array(cropped_img), normalizer)\n",
    "    \n",
    "    # create input tensor\n",
    "    input_tensor, image_float_np = prepare_input(np.array(norm_img))\n",
    "    \n",
    "    #generate predictions\n",
    "    masks, boxes, labels, scores = get_outputs_nms(input_tensor, model, 0.6, 0.25)\n",
    "    cored_pred = 0\n",
    "    cg_pred = 0\n",
    "    diffuse_pred = 0\n",
    "    caa_pred = 0\n",
    "    label_list= []\n",
    "    \n",
    "    # get ground truth label\n",
    "    cored_gt = row[\"cored\"]\n",
    "    diffuse_gt = row[\"diffuse\"]\n",
    "    CAA_gt =  row[\"CAA\"]\n",
    "    \n",
    "    \n",
    "    #zoomed_blob_coords = [260, 88,260+ 92,88+ 68]\n",
    "    #print(zoomed_blob_coords)\n",
    "    for j in range(len(boxes)):\n",
    "        box1 = torch.tensor([zoomed_blob_coords], dtype=torch.float)\n",
    "        box2 = torch.tensor([boxes[j]], dtype=torch.float)\n",
    "        iou = bops.box_iou(box1, box2)\n",
    "        #print(iou)\n",
    "        if iou>0:\n",
    "            label_list.append(labels[j])\n",
    "    return {\"all_labels\":labels, \"matched_labels\" :label_list,  \"scores\":scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for all elements of dataframe \n",
    "merged_data[\"pred_labels\"] =merged_data.apply(lambda row:  run_prediction(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all predictions\n",
    "merged_data[\"all_pred_count\"]  = merged_data[\"pred_labels\"].apply(lambda l: Counter(l[\"all_labels\"]))  \n",
    "# count all matched labels\n",
    "merged_data[\"matched_count\"]  = merged_data[\"pred_labels\"].apply(lambda l: Counter(l[\"matched_labels\"]))\n",
    "\n",
    "# Check class wise match\n",
    "merged_data[\"matched_cored_pred\"] =merged_data[\"matched_count\"].apply(lambda l: 0 if l.find(\"Cored\")==-1 else 1)\n",
    "merged_data[\"matched_diffuse_pred\"] =merged_data[\"matched_count\"].apply(lambda l: 0 if l.find(\"Diffuse\")==-1 else 1)\n",
    "merged_data[\"matched_cg_pred\"] =merged_data[\"matched_count\"].apply(lambda l: 0 if l.find(\"Coarse-Grained\")==-1 else 1)\n",
    "merged_data[\"matched_caa_pred\"] =merged_data[\"matched_count\"].apply(lambda l: 0 if l.find(\"CAA\")==-1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep dataset with atleast 1 match\n",
    "one_matched = merged_data[ (merged_data[\"matched_diffuse_pred\"]>0) | (merged_data[\"matched_cg_pred\"]>0) | (merged_data[\"matched_caa_pred\"]>0) | (merged_data[\"matched_cored_pred\"]>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter neg and notsure\n",
    "one_matched = one_matched[one_matched[\"negative\"]<1]\n",
    "one_matched = one_matched[one_matched[\"notsure\"]<=0]\n",
    "\n",
    "# format variables\n",
    "one_matched[\"diffuse_bool\"] = one_matched[\"diffuse_bool\"].astype(int)\n",
    "one_matched[\"cored_bool\"] = one_matched[\"cored_bool\"].astype(int)\n",
    "one_matched[\"cored_bool\"] = one_matched[\"cored_bool\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, f1 score for Diffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6148359486447932, 0.7329931972789115, 0.6687354538401862, None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(one_matched[\"diffuse_bool\"], one_matched[\"matched_diffuse_pred\"])\n",
    "precision_recall_fscore_support(one_matched[\"diffuse_bool\"], one_matched[\"matched_diffuse_pred\"],average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, f1 score for Cored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8425369054127939, 0.19739960289502337, 0.31985885527476515, None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(one_matched[\"cored_bool\"], one_matched[\"matched_cored_pred\"])\n",
    "precision_recall_fscore_support(one_matched[\"cored_bool\"], one_matched[\"matched_cored_pred\"],average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, f1 score for Cored + Coarse-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7290297642056436, 0.2415935438416704, 0.36291913214990135, None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_matched[\"matched_cored_cg\"] = one_matched.apply(lambda row: max(row[\"matched_cored_pred\"] , row[\"matched_cg_pred\"]), axis=1)\n",
    "precision_recall_fscore_support(one_matched[\"cored_bool\"], one_matched[\"matched_cored_cg\"], average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, f1 score for CAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3239760699493787, 0.6305418719211823, 0.42802857577139386, None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "one_matched[\"CAA_bool\"] = one_matched[\"CAA_bool\"].astype(int)\n",
    "confusion_matrix(one_matched[\"CAA_bool\"], one_matched[\"matched_caa_pred\"])\n",
    "precision_recall_fscore_support(one_matched[\"CAA_bool\"], one_matched[\"matched_caa_pred\"], average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "merged_data.to_csv(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/data/CERAD/predictions_ucdavis_final.csv\")\n",
    "one_matched.to_csv(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/data/CERAD/predictions_ucdavis_matched.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfold_amy_plaque1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
