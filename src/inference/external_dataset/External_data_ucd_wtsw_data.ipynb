{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code runs predictions for external dataset 2 and match the output with ground truth of external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "#from models.model_mrcnn import _default_mrcnn_config, build_default\n",
    "import torchvision\n",
    "import torch\n",
    "from models_pytorch_lightning.generalized_mask_rcnn_pl import LitMaskRCNN\n",
    "import torchvision.ops.boxes as bops\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyvips as Vips\n",
    "from Reinhard import Reinhard\n",
    "from openslide import OpenSlide\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from glob import glob\n",
    "import random\n",
    "from infer_utils import get_outputs_nms, prepare_input\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:55: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'rpn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['rpn'])`.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'roi_heads' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['roi_heads'])`.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'transform' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transform'])`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/runpod_mrcnn_models/yp2mf3i8_epoch=108-step=872.ckpt\"\n",
    "model = LitMaskRCNN.load_from_checkpoint(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval().to(device)\n",
    "class_names = ['Cored', 'Diffuse', 'Coarse-Grained', 'CAA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to preprocess external data input to make it as same format as trained model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize normalization based on the reference image\n",
    "def initialize_normalizer(ref_slide_path):\n",
    "    print(\"Initializing normalization with reference slide:\", ref_slide_path)\n",
    "    ref_image = Vips.Image.new_from_file(ref_slide_path)\n",
    "    normalizer = Reinhard()\n",
    "    normalizer.fit(ref_image)\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "# Function to normalize a single slide\n",
    "def normalize_slide(target_image, normalizer, filepath, idx, output_dir):\n",
    "    #print(\"Normalizing slide:\", target_slide_path)\n",
    "    # Load the target slide\n",
    "    #target_image = Vips.Image.new_from_file(target_slide_path)\n",
    "    target_image = Vips.Image.new_from_memory(target_image.data, target_image.shape[1], target_image.shape[0], 3, \"uchar\")\n",
    "    # Perform normalization\n",
    "    normalized_image = normalizer.transform(target_image)\n",
    "    # Generate the output file path\n",
    "    #file_name = os.path.basename(target_slide_path).replace(\".mrxs\", \".tif\")\n",
    "    output_file_path = os.path.join(output_dir, idx+\"_\" +filepath.split(\"/\")[-1])\n",
    "    # Save the normalized image\n",
    "    normalized_image.write_to_file(output_file_path)\n",
    "    return normalized_image\n",
    "\n",
    "def normalize_slide(target_image, normalizer):\n",
    "    target_image = Vips.Image.new_from_memory(target_image.data, target_image.shape[1], target_image.shape[0], 3, \"uchar\")\n",
    "    # Perform normalization\n",
    "    normalized_image = normalizer.transform(target_image)\n",
    "    return normalized_image\n",
    "\n",
    "def change_to_numpy_array(input_tensor):\n",
    "    input_tensor_cpu = input_tensor.squeeze(0).cpu()  # Remove batch dimension and move to CPU if necessary\n",
    "    # Convert to NumPy array\n",
    "    numpy_array = input_tensor_cpu.permute(1, 2, 0).numpy()  # Convert to HWC format for visualization\n",
    "    # Scale back to 0-255 range if needed\n",
    "    numpy_array = (numpy_array * 255).astype(np.uint8)\n",
    "    return numpy_array\n",
    "\n",
    "\n",
    "def convert_bbox(bbox):\n",
    "    \"\"\"\n",
    "    Convert bounding box from [center_x, center_y, width, height] \n",
    "    to [x_min, y_min, x_max, y_max].\n",
    "\n",
    "    Parameters:\n",
    "        center_x (float): Center x-coordinate (normalized, 0-1).\n",
    "        center_y (float): Center y-coordinate (normalized, 0-1).\n",
    "        w (float): Width of the bounding box (normalized, 0-1).\n",
    "        h (float): Height of the bounding box (normalized, 0-1).\n",
    "    \n",
    "    Returns:\n",
    "        list: [x_min, y_min, x_max, y_max] (normalized)\n",
    "    \"\"\"\n",
    "    center_x, center_y, w, h = bbox\n",
    "    x_min = center_x - w / 2\n",
    "    y_min = center_y - h / 2\n",
    "    x_max = center_x + w / 2\n",
    "    y_max = center_y + h / 2\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "def zoom_and_crop(image, bboxes, labels, zoom_factor=2, crop_size=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Zoom the image and crop it into patches of specified size while adjusting bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy array): Original image of size (1536, 1536, 3)\n",
    "        bboxes (list): List of normalized bounding boxes [[x_min, y_min, x_max, y_max], ...]\n",
    "        zoom_factor (int): Factor to zoom the image.\n",
    "        crop_size (tuple): Size of the crop (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        crops (list): List of cropped images.\n",
    "        cropped_bboxes (list): List of bounding boxes adjusted for each crop.\n",
    "    \"\"\"\n",
    "    # Step 1: Zoom the image\n",
    "    original_h, original_w = image.shape[:2]\n",
    "    zoomed_h, zoomed_w = int(original_h * zoom_factor), int(original_w * zoom_factor)\n",
    "    zoomed_image = cv2.resize(image, (zoomed_w, zoomed_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # Step 2: Scale bounding boxes to match the zoomed image\n",
    "    scaled_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        x_min = int(x_min * zoomed_w)\n",
    "        y_min = int(y_min * zoomed_h)\n",
    "        x_max = int(x_max * zoomed_w)\n",
    "        y_max = int(y_max * zoomed_h)\n",
    "        scaled_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    # Step 3: Generate crops and adjust bounding boxes\n",
    "    crops = []\n",
    "    cropped_bboxes = []\n",
    "    cropped_labels = []\n",
    "\n",
    "    crop_h, crop_w = crop_size\n",
    "\n",
    "    for y in range(0, zoomed_h, crop_h):\n",
    "        for x in range(0, zoomed_w, crop_w):\n",
    "            # Define the crop region\n",
    "            x1, y1 = x, y\n",
    "            x2, y2 = min(x + crop_w, zoomed_w), min(y + crop_h, zoomed_h)\n",
    "            crop = zoomed_image[y1:y2, x1:x2]\n",
    "\n",
    "            # Adjust bounding boxes for this crop\n",
    "            bboxes_in_crop = []\n",
    "            labels_in_crop = []\n",
    "            for bbox,label in zip(scaled_bboxes,labels):\n",
    "                bx_min, by_min, bx_max, by_max = bbox\n",
    "\n",
    "                # Check if the bounding box intersects with the crop\n",
    "                if bx_max > x1 and bx_min < x2 and by_max > y1 and by_min < y2:\n",
    "                    # Adjust the bounding box to the crop's coordinate system\n",
    "                    new_x_min = max(bx_min - x1, 0)\n",
    "                    new_y_min = max(by_min - y1, 0)\n",
    "                    new_x_max = min(bx_max - x1, crop_w)\n",
    "                    new_y_max = min(by_max - y1, crop_h)\n",
    "                    bboxes_in_crop.append([new_x_min, new_y_min, new_x_max, new_y_max])\n",
    "                    labels_in_crop.append(label)\n",
    "            # Save the crop and corresponding bounding boxes\n",
    "            crops.append(crop)\n",
    "            cropped_bboxes.append(bboxes_in_crop)\n",
    "            cropped_labels.append(labels_in_crop)\n",
    "\n",
    "    return crops, cropped_bboxes,cropped_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load external dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/External_Dataset-Others/data/custom/labels\"\n",
    "label_list = os.listdir(path)\n",
    "img_folder =  \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/External_Dataset-Others/data/custom/images\"\n",
    "img_names =  [ os.path.join(img_folder,label_list[i].replace(\".txt\",\".jpg\")) for i in range(len(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing normalization with reference slide: /gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/XE07-047_1_AmyB_1/image/XE07-047_1_AmyB_1_8381x_120830y_image.png\n"
     ]
    }
   ],
   "source": [
    "# Path to the reference slide\n",
    "ref_slide_path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/XE07-047_1_AmyB_1/image/XE07-047_1_AmyB_1_8381x_120830y_image.png\"\n",
    "# Initialize the normalizer\n",
    "normalizer = initialize_normalizer(ref_slide_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run external dataset on trained model to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_class ={0:\"CAA\",1:\"Cored\"}\n",
    "img_names_list_norm = []\n",
    "act_label_list_norm = []\n",
    "pred_label_list_norm =[ ]\n",
    "\n",
    "for i in range(len(img_names)):\n",
    "    # open image\n",
    "    image = Image.open(img_names[i])\n",
    "    label_path = os.path.join(path, label_list[i])\n",
    "    #load ground truth of external dataset\n",
    "    gt_boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
    "    gt_labels = gt_boxes[:,0]\n",
    "    gt_bboxes = gt_boxes[:,1:]\n",
    "    gt_bboxes_form = [ convert_bbox(box) for box in gt_bboxes ]\n",
    "    # preprocess input by zooming and cropping\n",
    "    crops, cropped_bboxes, cropped_labels =  zoom_and_crop(np.array(image), gt_bboxes_form,gt_labels,  zoom_factor=2, crop_size=(1024, 1024))\n",
    "    for crop, cropped_bbox, cropped_label in zip( crops, cropped_bboxes, cropped_labels):\n",
    "        if len(cropped_bbox)>0:\n",
    "            # normalize the image\n",
    "            image_inp_norm =  normalize_slide(np.array(crop), normalizer)\n",
    "            # create tensor input\n",
    "            input_tensor, image_float_np = prepare_input(np.array(image_inp_norm))\n",
    "            # get model predictions\n",
    "            masks, pred_boxes, pred_labels, pred_scores = get_outputs_nms(input_tensor, model, 0.6, 0.25)\n",
    "            # match ground truth and predicted bboxes and save\n",
    "            for j in range(len(cropped_bbox)):\n",
    "                for k in range(len(pred_boxes)):\n",
    "                    box1 = torch.tensor([cropped_bbox[j]], dtype=torch.float)\n",
    "                    box2 = torch.tensor([pred_boxes[k]], dtype=torch.float)\n",
    "                    iou = bops.box_iou(box1, box2)\n",
    "                    if iou>0:\n",
    "                        image_1024 = change_to_numpy_array(input_tensor)\n",
    "                        bgr_img = Image.fromarray(image_1024)\n",
    "                        draw = ImageDraw.Draw(bgr_img)\n",
    "                        draw.rectangle(cropped_bbox[j] ,outline='red')\n",
    "                        draw.text((cropped_bbox[j][0],cropped_bbox[j][1]) , \"GT: \"+map_class[cropped_label[j]], 'red',myFont)\n",
    "                        draw.rectangle(pred_boxes[k] ,outline='green')\n",
    "                        draw.text((pred_boxes[k][2],pred_boxes[k][3]), \"Pred: \"+pred_labels[k], 'green',myFont)\n",
    "                        img_name_output = img_names[i].split(\"/\")[-1]\n",
    "                        bgr_img.save(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/output_text_formatted_bbox\",str(j)+str(k)+\"_\"+\"GT_\"+map_class[cropped_label[j]] +\"_\"+\"pred_\"+pred_labels[k]+\"_\"+img_name_output))\n",
    "                        img_names_list_norm.append(img_names[i].split(\"/\")[-1])\n",
    "                        act_label_list_norm.append(cropped_label[j])\n",
    "                        pred_label_list_norm.append(pred_labels[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1857"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric of \"Cored\" and \"CAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_norm = pd.DataFrame({\"image_name\":img_names_list_norm,\"act_label\":act_label_list_norm, \"pred_label\":pred_label_list_norm})\n",
    "output_norm[\"act_label\"] = output_norm[\"act_label\"].apply(lambda l: \"CAA\" if l==0 else \"Cored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124,   9,   1, 135],\n",
       "       [  0,   0,   0,   0],\n",
       "       [ 46, 306, 150, 645],\n",
       "       [  0,   0,   0,   0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(output_norm[\"act_label\"], output_norm[\"pred_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.72941176, 0.        , 0.99337748, 0.        ]),\n",
       " array([0.46096654, 0.        , 0.13077594, 0.        ]),\n",
       " array([0.56492027, 0.        , 0.23112481, 0.        ]),\n",
       " array([ 269,    0, 1147,    0]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(output_norm[\"act_label\"], output_norm[\"pred_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric of pred - \"Cored\" + Coarse-Grained\" as \"Cored\" and \"CAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124,  10, 135],\n",
       "       [ 46, 456, 645],\n",
       "       [  0,   0,   0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"pred_label1\"] = output_norm[\"pred_label\"].apply(lambda l: \"Cored\" if l==\"Coarse-Grained\" else l)\n",
    "confusion_matrix(output_norm[\"act_label\"], output_norm[\"pred_label1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diffuse    780\n",
       "Cored      466\n",
       "CAA        170\n",
       "Name: pred_label1, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"pred_label1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.72941176, 0.97854077, 0.        ]),\n",
       " array([0.46096654, 0.39755885, 0.        ]),\n",
       " array([0.56492027, 0.56540608, 0.        ]),\n",
       " array([ 269, 1147,    0]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(output_norm[\"act_label\"], output_norm[\"pred_label1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "output_norm.to_csv(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/external_dataset_2_final_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfold_amy_plaque1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
