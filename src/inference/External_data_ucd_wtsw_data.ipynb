{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "#from models.model_mrcnn import _default_mrcnn_config, build_default\n",
    "import torchvision\n",
    "import torch\n",
    "from models_pytorch_lightning.generalized_mask_rcnn_pl import LitMaskRCNN\n",
    "import torchvision.ops.boxes as bops\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyvips as Vips\n",
    "from Reinhard import Reinhard\n",
    "from openslide import OpenSlide\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from glob import glob\n",
    "import random\n",
    "from infer_utils import get_outputs_nms, prepare_input\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:55: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'rpn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['rpn'])`.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'roi_heads' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['roi_heads'])`.\n",
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'transform' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transform'])`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/runpod_mrcnn_models/yp2mf3i8_epoch=108-step=872.ckpt\"\n",
    "model = LitMaskRCNN.load_from_checkpoint(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval().to(device)\n",
    "class_names = ['Cored', 'Diffuse', 'Coarse-Grained', 'CAA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/External_Dataset-Others/data/custom/labels\"\n",
    "label_list = os.listdir(path)\n",
    "img_folder =  \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/External_Dataset-Others/data/custom/images\"\n",
    "img_names =  [ os.path.join(img_folder,label_list[i].replace(\".txt\",\".jpg\")) for i in range(len(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize normalization based on the reference image\n",
    "def initialize_normalizer(ref_slide_path):\n",
    "    print(\"Initializing normalization with reference slide:\", ref_slide_path)\n",
    "    ref_image = Vips.Image.new_from_file(ref_slide_path)\n",
    "    normalizer = Reinhard()\n",
    "    normalizer.fit(ref_image)\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "# Function to normalize a single slide\n",
    "def normalize_slide(target_image, normalizer, filepath, idx, output_dir):\n",
    "    #print(\"Normalizing slide:\", target_slide_path)\n",
    "    # Load the target slide\n",
    "    #target_image = Vips.Image.new_from_file(target_slide_path)\n",
    "    target_image = Vips.Image.new_from_memory(target_image.data, target_image.shape[1], target_image.shape[0], 3, \"uchar\")\n",
    "    # Perform normalization\n",
    "    normalized_image = normalizer.transform(target_image)\n",
    "    # Generate the output file path\n",
    "    #file_name = os.path.basename(target_slide_path).replace(\".mrxs\", \".tif\")\n",
    "    output_file_path = os.path.join(output_dir, idx+\"_\" +filepath.split(\"/\")[-1])\n",
    "    # Save the normalized image\n",
    "    normalized_image.write_to_file(output_file_path)\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox(bbox):\n",
    "    \"\"\"\n",
    "    Convert bounding box from [center_x, center_y, width, height] \n",
    "    to [x_min, y_min, x_max, y_max].\n",
    "\n",
    "    Parameters:\n",
    "        center_x (float): Center x-coordinate (normalized, 0-1).\n",
    "        center_y (float): Center y-coordinate (normalized, 0-1).\n",
    "        w (float): Width of the bounding box (normalized, 0-1).\n",
    "        h (float): Height of the bounding box (normalized, 0-1).\n",
    "    \n",
    "    Returns:\n",
    "        list: [x_min, y_min, x_max, y_max] (normalized)\n",
    "    \"\"\"\n",
    "    center_x, center_y, w, h = bbox\n",
    "    x_min = center_x - w / 2\n",
    "    y_min = center_y - h / 2\n",
    "    x_max = center_x + w / 2\n",
    "    y_max = center_y + h / 2\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_and_crop(image, bboxes, labels, zoom_factor=2, crop_size=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Zoom the image and crop it into patches of specified size while adjusting bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy array): Original image of size (1536, 1536, 3)\n",
    "        bboxes (list): List of normalized bounding boxes [[x_min, y_min, x_max, y_max], ...]\n",
    "        zoom_factor (int): Factor to zoom the image.\n",
    "        crop_size (tuple): Size of the crop (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        crops (list): List of cropped images.\n",
    "        cropped_bboxes (list): List of bounding boxes adjusted for each crop.\n",
    "    \"\"\"\n",
    "    # Step 1: Zoom the image\n",
    "    original_h, original_w = image.shape[:2]\n",
    "    zoomed_h, zoomed_w = int(original_h * zoom_factor), int(original_w * zoom_factor)\n",
    "    zoomed_image = cv2.resize(image, (zoomed_w, zoomed_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # Step 2: Scale bounding boxes to match the zoomed image\n",
    "    scaled_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        x_min = int(x_min * zoomed_w)\n",
    "        y_min = int(y_min * zoomed_h)\n",
    "        x_max = int(x_max * zoomed_w)\n",
    "        y_max = int(y_max * zoomed_h)\n",
    "        scaled_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    # Step 3: Generate crops and adjust bounding boxes\n",
    "    crops = []\n",
    "    cropped_bboxes = []\n",
    "    cropped_labels = []\n",
    "\n",
    "    crop_h, crop_w = crop_size\n",
    "\n",
    "    for y in range(0, zoomed_h, crop_h):\n",
    "        for x in range(0, zoomed_w, crop_w):\n",
    "            # Define the crop region\n",
    "            x1, y1 = x, y\n",
    "            x2, y2 = min(x + crop_w, zoomed_w), min(y + crop_h, zoomed_h)\n",
    "            crop = zoomed_image[y1:y2, x1:x2]\n",
    "\n",
    "            # Adjust bounding boxes for this crop\n",
    "            bboxes_in_crop = []\n",
    "            labels_in_crop = []\n",
    "            for bbox,label in zip(scaled_bboxes,labels):\n",
    "                bx_min, by_min, bx_max, by_max = bbox\n",
    "\n",
    "                # Check if the bounding box intersects with the crop\n",
    "                if bx_max > x1 and bx_min < x2 and by_max > y1 and by_min < y2:\n",
    "                    # Adjust the bounding box to the crop's coordinate system\n",
    "                    new_x_min = max(bx_min - x1, 0)\n",
    "                    new_y_min = max(by_min - y1, 0)\n",
    "                    new_x_max = min(bx_max - x1, crop_w)\n",
    "                    new_y_max = min(by_max - y1, crop_h)\n",
    "                    bboxes_in_crop.append([new_x_min, new_y_min, new_x_max, new_y_max])\n",
    "                    labels_in_crop.append(label)\n",
    "            # Save the crop and corresponding bounding boxes\n",
    "            crops.append(crop)\n",
    "            cropped_bboxes.append(bboxes_in_crop)\n",
    "            cropped_labels.append(labels_in_crop)\n",
    "\n",
    "    return crops, cropped_bboxes,cropped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs = glob(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/normalized_data/images\",\"*\"))\n",
    "#lbls = glob(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/normalized_data/bboxes\",\"*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/External_Dataset-Others/data/custom/labels\"\n",
    "label_list = os.listdir(path)\n",
    "\n",
    "img_folder =  \"/home/mahirwar/Desktop/Monika/npsad_data/vivek/Datasets/External_Dataset-Others/data/custom/images\"\n",
    "img_names =  [ os.path.join(img_folder,label_list[i].replace(\".txt\",\".jpg\")) for i in range(len(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing normalization with reference slide: /gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/XE07-047_1_AmyB_1/image/XE07-047_1_AmyB_1_8381x_120830y_image.png\n"
     ]
    }
   ],
   "source": [
    "# Path to the reference slide\n",
    "ref_slide_path = \"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/amyb_wsi_v2/XE07-047_1_AmyB_1/image/XE07-047_1_AmyB_1_8381x_120830y_image.png\"\n",
    "# Initialize the normalizer\n",
    "normalizer = initialize_normalizer(ref_slide_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_slide(target_image, normalizer):\n",
    "    target_image = Vips.Image.new_from_memory(target_image.data, target_image.shape[1], target_image.shape[0], 3, \"uchar\")\n",
    "    # Perform normalization\n",
    "    normalized_image = normalizer.transform(target_image)\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_numpy_array(input_tensor):\n",
    "    input_tensor_cpu = input_tensor.squeeze(0).cpu()  # Remove batch dimension and move to CPU if necessary\n",
    "    # Convert to NumPy array\n",
    "    numpy_array = input_tensor_cpu.permute(1, 2, 0).numpy()  # Convert to HWC format for visualization\n",
    "    # Scale back to 0-255 range if needed\n",
    "    numpy_array = (numpy_array * 255).astype(np.uint8)\n",
    "    return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a font\n",
    "myFont = ImageFont.truetype('FreeMono.ttf', 30)\n",
    "\n",
    "# Add text\n",
    "text = \"Hello, World!\"\n",
    "text_color = (255, 255, 255)  # White\n",
    "text_position = (10, 10)  # Top-left corner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_class ={0:\"CAA\",1:\"Cored\"}\n",
    "img_names_list_norm = []\n",
    "act_label_list_norm = []\n",
    "pred_label_list_norm =[ ]\n",
    "for i in range(len(img_names)):\n",
    "    image = Image.open(img_names[i])\n",
    "    label_path = os.path.join(path, label_list[i])\n",
    "    gt_boxes = np.loadtxt(label_path).reshape(-1, 5)\n",
    "    gt_labels = gt_boxes[:,0]\n",
    "    gt_bboxes = gt_boxes[:,1:]\n",
    "    gt_bboxes_form = [ convert_bbox(box) for box in gt_bboxes ]\n",
    "    crops, cropped_bboxes, cropped_labels =  zoom_and_crop(np.array(image), gt_bboxes_form,gt_labels,  zoom_factor=2, crop_size=(1024, 1024))\n",
    "    for crop, cropped_bbox, cropped_label in zip( crops, cropped_bboxes, cropped_labels):\n",
    "        if len(cropped_bbox)>0:\n",
    "            image_inp_norm =  normalize_slide(np.array(crop), normalizer)\n",
    "            input_tensor, image_float_np = prepare_input(np.array(image_inp_norm))\n",
    "            masks, pred_boxes, pred_labels, pred_scores = get_outputs_nms(input_tensor, model, 0.6, 0.25)\n",
    "            for j in range(len(cropped_bbox)):\n",
    "                for k in range(len(pred_boxes)):\n",
    "                    box1 = torch.tensor([cropped_bbox[j]], dtype=torch.float)\n",
    "                    box2 = torch.tensor([pred_boxes[k]], dtype=torch.float)\n",
    "                    iou = bops.box_iou(box1, box2)\n",
    "                    if iou>0:\n",
    "                        image_1024 = change_to_numpy_array(input_tensor)\n",
    "                        bgr_img = Image.fromarray(image_1024)\n",
    "                        draw = ImageDraw.Draw(bgr_img)\n",
    "                        draw.rectangle(cropped_bbox[j] ,outline='red')\n",
    "                        draw.text((cropped_bbox[j][0],cropped_bbox[j][1]) , \"GT: \"+map_class[cropped_label[j]], 'red',myFont)\n",
    "                        draw.rectangle(pred_boxes[k] ,outline='green')\n",
    "                        draw.text((pred_boxes[k][2],pred_boxes[k][3]), \"Pred: \"+pred_labels[k], 'green',myFont)\n",
    "                        img_name_output = img_names[i].split(\"/\")[-1]\n",
    "                        bgr_img.save(os.path.join(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/output_text_formatted_bbox\",str(j)+str(k)+\"_\"+\"GT_\"+map_class[cropped_label[j]] +\"_\"+\"pred_\"+pred_labels[k]+\"_\"+img_name_output))\n",
    "                        img_names_list_norm.append(img_names[i].split(\"/\")[-1])\n",
    "                        act_label_list_norm.append(cropped_label[j])\n",
    "                        pred_label_list_norm.append(pred_labels[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1857"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1855"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(act_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1855"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"act_label\":act_label_list, \"pred_label\":pred_label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"act_label\"] = output[\"act_label\"].apply(lambda l: \"CAA\" if l==0 else \"Cored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Cored</td>\n",
       "      <td>Cored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Cored</td>\n",
       "      <td>Cored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act_label pred_label\n",
       "26       CAA        CAA\n",
       "39       CAA        CAA\n",
       "45       CAA        CAA\n",
       "50     Cored      Cored\n",
       "53       CAA        CAA\n",
       "61     Cored      Cored\n",
       "62       CAA        CAA\n",
       "67       CAA        CAA\n",
       "68       CAA        CAA\n",
       "71       CAA        CAA\n",
       "80       CAA        CAA"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[(output[\"act_label\"]==output[\"pred_label\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cored    1180\n",
       "CAA       675\n",
       "Name: act_label, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"act_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diffuse           1841\n",
       "CAA                  9\n",
       "Coarse-Grained       3\n",
       "Cored                2\n",
       "Name: pred_label, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"pred_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_norm = pd.DataFrame({\"image_name\":img_names_list_norm,\"act_label\":act_label_list_norm, \"pred_label\":pred_label_list_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_norm[\"act_label\"] = output_norm[\"act_label\"].apply(lambda l: \"CAA\" if l==0 else \"Cored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>act_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cw18-015_0_10_15.jpg</td>\n",
       "      <td>Cored</td>\n",
       "      <td>Diffuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cw18-015_0_10_15.jpg</td>\n",
       "      <td>Cored</td>\n",
       "      <td>Coarse-Grained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cw18-015_0_10_15.jpg</td>\n",
       "      <td>Cored</td>\n",
       "      <td>Coarse-Grained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cw18-015_0_10_15.jpg</td>\n",
       "      <td>Cored</td>\n",
       "      <td>Coarse-Grained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cw17-018_0_8_27.jpg</td>\n",
       "      <td>Cored</td>\n",
       "      <td>Coarse-Grained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>NA4916UCDtemporalgyri _4G8_0_18_17.jpg</td>\n",
       "      <td>Cored</td>\n",
       "      <td>Diffuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>cw17-022_0_19_15.jpg</td>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>cw17-022_0_19_15.jpg</td>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>cw17-022_0_19_15.jpg</td>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>cw17-022_0_19_15.jpg</td>\n",
       "      <td>CAA</td>\n",
       "      <td>CAA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_name act_label      pred_label\n",
       "0                       cw18-015_0_10_15.jpg     Cored         Diffuse\n",
       "1                       cw18-015_0_10_15.jpg     Cored  Coarse-Grained\n",
       "2                       cw18-015_0_10_15.jpg     Cored  Coarse-Grained\n",
       "3                       cw18-015_0_10_15.jpg     Cored  Coarse-Grained\n",
       "4                        cw17-018_0_8_27.jpg     Cored  Coarse-Grained\n",
       "...                                      ...       ...             ...\n",
       "1411  NA4916UCDtemporalgyri _4G8_0_18_17.jpg     Cored         Diffuse\n",
       "1412                    cw17-022_0_19_15.jpg       CAA             CAA\n",
       "1413                    cw17-022_0_19_15.jpg       CAA             CAA\n",
       "1414                    cw17-022_0_19_15.jpg       CAA             CAA\n",
       "1415                    cw17-022_0_19_15.jpg       CAA             CAA\n",
       "\n",
       "[1416 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124,   9,   1, 135],\n",
       "       [  0,   0,   0,   0],\n",
       "       [ 46, 306, 150, 645],\n",
       "       [  0,   0,   0,   0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "confusion_matrix(output_norm[\"act_label\"], output_norm[\"pred_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.72941176, 0.        , 0.99337748, 0.        ]),\n",
       " array([0.46096654, 0.        , 0.13077594, 0.        ]),\n",
       " array([0.56492027, 0.        , 0.23112481, 0.        ]),\n",
       " array([ 269,    0, 1147,    0]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(output_norm[\"act_label\"], output_norm[\"pred_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_norm[\"pred_label1\"] = output_norm[\"pred_label\"].apply(lambda l: \"Cored\" if l==\"Coarse-Grained\" or l==\"Diffuse\" else l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cored    1246\n",
       "CAA       170\n",
       "Name: pred_label1, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"pred_label1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cored    1147\n",
       "CAA       269\n",
       "Name: act_label, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"act_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 124,  145],\n",
       "       [  46, 1101]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"pred_label1\"] = output_norm[\"pred_label\"].apply(lambda l: \"Cored\" if l==\"Coarse-Grained\" or l==\"Diffuse\" else l)\n",
    "confusion_matrix(output_norm[\"act_label\"], output_norm[\"pred_label1\"],labels=[\"CAA\", \"Cored\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.72941176, 0.88362761]),\n",
       " array([0.46096654, 0.95989538]),\n",
       " array([0.56492027, 0.92018387]),\n",
       " array([ 269, 1147]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(output_norm[\"act_label\"], output_norm[\"pred_label1\"],labels=[\"CAA\", \"Cored\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124,  10, 135],\n",
       "       [ 46, 456, 645],\n",
       "       [  0,   0,   0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"pred_label1\"] = output_norm[\"pred_label\"].apply(lambda l: \"Cored\" if l==\"Coarse-Grained\" else l)\n",
    "confusion_matrix(output_norm[\"act_label\"], output_norm[\"pred_label1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diffuse    780\n",
       "Cored      466\n",
       "CAA        170\n",
       "Name: pred_label1, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_norm[\"pred_label1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahirwar/miniconda3/envs/kfold_amy_plaque1/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.72941176, 0.97854077, 0.        ]),\n",
       " array([0.46096654, 0.39755885, 0.        ]),\n",
       " array([0.56492027, 0.56540608, 0.        ]),\n",
       " array([ 269, 1147,    0]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(output_norm[\"act_label\"], output_norm[\"pred_label1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_norm.to_csv(\"/gladstone/finkbeiner/steve/work/data/npsad_data/vivek/Datasets/External_Dataset-Others/external_dataset_2_final_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfold_amy_plaque1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
